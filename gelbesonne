            print(f'scraping tweets with keyword: "{keyword}" ...')
            try:
                os.system(
                    f'snscrape twitter-{keyword_user_search_param} "{keyword} since:{since} until:{until} lang:{lang}" > {output_path}')
            except Exception as err:
                print(f"SNSCRAPE ERROR: {err}")



import snscrape.modules.twitter.

for tweet in snscrape.modules.twitter.TwitterProfileScraper('username').get_items():
	# Do something with the tweet object, e.g.
	print(tweet.url)

import re

text = "@RayFranco is answering to @jjconti, this is a real '@username83' but this is an@email.com, and this is a @probablyfaketwitterusername";

result = re.findall("(^|[^@\w])@(\w{1,15})", text)

print(result);

xxxxxxxxxxxxxxxxxxxx


import re
import pandas as pd

trump = pd.read_csv('https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6')
trump.text = trump.apply(lambda row: re.sub(r"http\S+", "", row.text).lower(), 1)
trump.text = trump.apply(lambda row: " ".join(filter(lambda x:x[0]!="@", row.text.split())), 1)
trump.text = trump.apply(lambda row: " ".join(re.sub("[^a-zA-Z]+", " ", row.text).split()), 1)
trump = trump.loc[(trump.isRetweet == "f") & (trump.text != ""), :]
timestamps = trump.date.to_list()
tweets = trump.text.to_list()

https://pypi.org/project/keybert/

https://github.com/prachiprakash26/Keyword_Extractor_Python

https://gist.github.com/mahmoud/237eb20108b5805aed5f


xxxxxxxxxxxxxxxxxxxxxx


from transformers import pipeline
ner = pipeline('ner', model='xlm-roberta-large-finetuned-conll03-german')
ner('Ich bin Luisa aus Berlin.')




##########
import requests

from bs4 import BeautifulSoup

def extract_telegram(url):
  page = requests.get(url)
  soup = BeautifulSoup(page.content, 'html.parser')

  numbers = soup.find('div', class_='tgme_page_extra').contents[0]
  
  if "members" in numbers:
    n_all = int(numbers.split(",")[0].replace(" members", "").replace(" ", ""))
    n_online = int(numbers.split(",")[1].replace(" online", "").replace(" ", ""))
  elif "subscribers" in numbers:
   n_all = int(numbers.replace(" subscribers", "").replace(" ", ""))
   n_online = - 1

  else:
    n_all = -1
    n_online = -1
  return(n_all, n_online)

list_urls = ['']
for url in list_urls:
  
  n_all, n_online = extract_telegram(url)
  print(n_all)
  print(n_online)
  
  
###

 !pip install schedule
import schedule3
import time

def job():
    print("I'm working...")

    list_urls = [
schedule.every(30).seconds.do(job)


while True:
    schedule.run_pending()
    time.sleep(1)


list_expressions = "gut|ich" df[df.texte.str.contains(list_expressions, regex=True)]

https://maxbundscherer.github.io/telegram-analysis/


https://towardsdatascience.com/building-a-social-network-from-the-news-using-graph-theory-by-marcell-ferencz-9155d314e77f

RETWEET_REGEX = r'^.*\brt\b\s+@([a-z0-9_]{1,15})' #'^rt @([a-z0-9_]{1,15})'REPLY_REGEX = r'^@([a-z0-9_]{1,15})'USERS_REGEX = r'@([a-z0-9_]{1,15})'#HASHTAGS_REGEX = r'#([a-z0-9_]{1,15})'#HASHTAGS_REGEX = r'(?:\W)#([a-z0-9_]{1,15})'HASHTAGS_REGEX = "\B#(\w*[a-zA-Z]+\w*)"RT_HEADER_REGEX = r".*(?:\brt\b)\s+@.*?:?" #".*(?:rt|RT)\s+@.*?:"EXCESSIVE_SPACES_REGEX = "\s+"#source: https://gist.github.com/gruber/249502#URL_REGEX = r'(?i)\b((?:[a-z][\w-]+:(?:/{1,3}|[a-z0-9%])|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?«»“”‘’]))'URL_REGEX = r'(\b((https?://|www\.)|[a-z0-9\.-]+?\.(com|co\.uk|org|net|info|ca)(?=[/ \W\b]))[^ \t\r\n<>]*?(?=(([\'\xe2\x80\x9c".?!,:;]|&(amp|lt|gt|quot);)+?)?(\.\.+|[<>]|\s|$)))'regh = re.compile(RT_HEADER_REGEX, flags=re.IGNORECASE)exs = re.compile(EXCESSIVE_SPACES_REGEX)
RE_URL = re.compile(URL_REGEX) if self.text: return [url[0] for url in RE_URL.findall(self.text)]
